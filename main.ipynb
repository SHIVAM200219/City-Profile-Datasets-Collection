{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection of various useful parameters of more than 200 cities across the world"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I have extracted a list of more than 200 cities having parameters like Quality of Life Index, Purchasing Power Index, Safety Index, Health Care Index, Cost of Living Index, Property Price to Income Ratio, Trafic Commute Time Index, Pollution Index & Climate Index through web scraping from the url - \"https://www.numbeo.com/quality-of-life/rankings_current.jsp\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been written to quality_of_life_indices.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "# Send a GET request to the URL\n",
    "url = \"https://www.numbeo.com/quality-of-life/rankings_current.jsp\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the table containing the data\n",
    "table = soup.find(\"table\", id=\"t2\")\n",
    "\n",
    "# Initialize an empty list to store the data\n",
    "data = []\n",
    "\n",
    "# Iterate through each row in the table body\n",
    "for row in table.find(\"tbody\").find_all(\"tr\"):\n",
    "    # Initialize an empty dictionary to store the row data\n",
    "    row_data = {}\n",
    "    \n",
    "    # Extract data from each column in the row\n",
    "    columns = row.find_all(\"td\")\n",
    "    row_data[\"City\"] = columns[1].text.strip()\n",
    "    row_data[\"Quality of Life Index\"] = columns[2].text.strip()\n",
    "    row_data[\"Purchasing Power Index\"] = columns[3].text.strip()\n",
    "    row_data[\"Safety Index\"] = columns[4].text.strip()\n",
    "    row_data[\"Health Care Index\"] = columns[5].text.strip()\n",
    "    row_data[\"Cost of Living Index\"] = columns[6].text.strip()\n",
    "    row_data[\"Property Price to Income Ratio\"] = columns[7].text.strip()\n",
    "    row_data[\"Traffic Commute Time Index\"] = columns[8].text.strip()\n",
    "    row_data[\"Pollution Index\"] = columns[9].text.strip()\n",
    "    row_data[\"Climate Index\"] = columns[10].text.strip()\n",
    "    \n",
    "    # Append the row data to the list\n",
    "    data.append(row_data)\n",
    "\n",
    "# Define the CSV file name\n",
    "csv_file = \"quality_of_life_indices.csv\"\n",
    "\n",
    "# Write the scraped data to the CSV file\n",
    "with open(csv_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    fieldnames = [\"City\", \"Quality of Life Index\", \"Purchasing Power Index\", \"Safety Index\",\n",
    "                  \"Health Care Index\", \"Cost of Living Index\", \"Property Price to Income Ratio\",\n",
    "                  \"Traffic Commute Time Index\", \"Pollution Index\", \"Climate Index\"]\n",
    "    writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "    \n",
    "    # Write the header\n",
    "    writer.writeheader()\n",
    "    \n",
    "    # Write the data\n",
    "    writer.writerows(data)\n",
    "\n",
    "print(f\"Data has been written to {csv_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some Data cleaning performed for City column to have uniform representation across the task.\n",
    "\n",
    "Country column extracted from the City."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv('quality_of_life_indices.csv')\n",
    "\n",
    "# Define a function to extract city and country\n",
    "def extract_city_country(city_country):\n",
    "    parts = city_country.split(',')\n",
    "    if len(parts) == 1:\n",
    "        return parts[0].strip(), ''\n",
    "    elif len(parts) == 2:\n",
    "        return parts[0].strip(), parts[1].strip()\n",
    "    else:\n",
    "        return parts[0].strip(), parts[-1].strip()\n",
    "\n",
    "# Apply the function to the 'City' column and create new 'city' and 'country' columns\n",
    "df['city'], df['country'] = zip(*df['City'].apply(extract_city_country))\n",
    "\n",
    "# Drop the original 'City' column\n",
    "df.drop(columns=['City'], inplace=True)\n",
    "\n",
    "# Reorder columns with 'city' and 'country' at the beginning\n",
    "df = df[['city', 'country'] + [col for col in df.columns if col != 'city' and col != 'country']]\n",
    "\n",
    "# Save the modified DataFrame to a new CSV file\n",
    "df.to_csv('modified_quality_of_life_indices.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collection of few more information for each cities like Population, Elevation & coordinates(useful for further extraction) through Geocoding API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "City information has been stored in city_information.csv\n"
     ]
    }
   ],
   "source": [
    "# Read data from modified_data.csv\n",
    "df = pd.read_csv('modified_quality_of_life_indices.csv')\n",
    "\n",
    "# Initialize lists to store city-wise information\n",
    "cities = df.iloc[:, 0].tolist()  # Get cities from the first column\n",
    "\n",
    "city_info = []\n",
    "\n",
    "# Iterate over each city\n",
    "for city in cities:\n",
    "    url = f\"https://geocoding-api.open-meteo.com/v1/search?name={city}&count=1&language=en&format=json\"\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    \n",
    "    # Check if data is available for the city\n",
    "    if 'results' in data and len(data['results']) > 0:\n",
    "        result = data['results'][0]\n",
    "        latitude = result['latitude']\n",
    "        longitude = result['longitude']\n",
    "        elevation = result['elevation'] if 'elevation' in result else None\n",
    "        population = result['population'] if 'population' in result else None\n",
    "        \n",
    "        city_info.append({\n",
    "            'City': city,\n",
    "            'Latitude': latitude,\n",
    "            'Longitude': longitude,\n",
    "            'Elevation': elevation,\n",
    "            'Population': population\n",
    "        })\n",
    "    else:\n",
    "        print(f\"No data found for {city}\")\n",
    "\n",
    "# Create a DataFrame from the collected city information\n",
    "city_info_df = pd.DataFrame(city_info)\n",
    "\n",
    "# Write the DataFrame to a new CSV file\n",
    "city_info_df.to_csv('city_information.csv', index=False)\n",
    "\n",
    "print(\"City information has been stored in city_information.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collection of weather parameters like Tempertaure, Humidity & Percipitation on monthly average basis through API.\n",
    "\n",
    "Every columns contains 12 space seperated values for each months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been written to weather_data.csv\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Function to extract monthly average weather data\n",
    "def extract_monthly_weather(lat, lon):\n",
    "    start_date = \"2023-01-01\"\n",
    "    end_date = \"2023-12-31\"\n",
    "    url = f\"https://archive-api.open-meteo.com/v1/era5?latitude={lat}&longitude={lon}&start_date={start_date}&end_date={end_date}&hourly=temperature_2m,relative_humidity_2m,precipitation\"\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    if 'hourly' in data and 'time' in data['hourly'] and 'temperature_2m' in data['hourly']:\n",
    "        hourly_temperatures = data['hourly']['temperature_2m']\n",
    "        hourly_humidity = data['hourly']['relative_humidity_2m']\n",
    "        hourly_precipitation = data['hourly']['precipitation']\n",
    "        \n",
    "        monthly_temperatures = {str(month): [] for month in range(1, 13)}\n",
    "        monthly_humidity = {str(month): [] for month in range(1, 13)}\n",
    "        monthly_precipitation = {str(month): [] for month in range(1, 13)}\n",
    "        \n",
    "        for time, temp, hum, precip in zip(data['hourly']['time'], hourly_temperatures, hourly_humidity, hourly_precipitation):\n",
    "            month = datetime.strptime(time, '%Y-%m-%dT%H:%M').month\n",
    "            monthly_temperatures[str(month)].append(temp)\n",
    "            monthly_humidity[str(month)].append(hum)\n",
    "            monthly_precipitation[str(month)].append(precip)\n",
    "        \n",
    "        monthly_mean_temperatures = [round(sum(values) / len(values), 2) if values else None for values in monthly_temperatures.values()]\n",
    "        monthly_mean_humidity = [round(sum(values) / len(values), 2) if values else None for values in monthly_humidity.values()]\n",
    "        monthly_mean_precipitation = [round(sum(values) / len(values), 2) if values else None for values in monthly_precipitation.values()]\n",
    "        \n",
    "        return monthly_mean_temperatures, monthly_mean_humidity, monthly_mean_precipitation\n",
    "    \n",
    "    return None, None, None\n",
    "\n",
    "# Read data from city_information.csv\n",
    "df = pd.read_csv('city_information.csv')\n",
    "\n",
    "# Initialize lists to store data\n",
    "cities = []\n",
    "latitudes = []\n",
    "longitudes = []\n",
    "monthly_mean_temperatures_list = []\n",
    "monthly_mean_humidity_list = []\n",
    "monthly_mean_precipitation_list = []\n",
    "\n",
    "# Iterate over each row in the dataframe\n",
    "for index, row in df.iterrows():\n",
    "    city = row['City']\n",
    "    lat = row['Latitude']\n",
    "    lon = row['Longitude']\n",
    "    \n",
    "    monthly_mean_temperatures, monthly_mean_humidity, monthly_mean_precipitation = extract_monthly_weather(lat, lon)\n",
    "    \n",
    "    if monthly_mean_temperatures is not None:\n",
    "        cities.append(city)\n",
    "        monthly_mean_temperatures_list.append(\" \".join(map(str, monthly_mean_temperatures)))\n",
    "        monthly_mean_humidity_list.append(\" \".join(map(str, monthly_mean_humidity)))\n",
    "        monthly_mean_precipitation_list.append(\" \".join(map(str, monthly_mean_precipitation)))\n",
    "\n",
    "# Create dataframe\n",
    "result_df = pd.DataFrame({\n",
    "    'City': cities,\n",
    "    'Monthly Mean Temperature': monthly_mean_temperatures_list,\n",
    "    'Monthly Mean Humidity': monthly_mean_humidity_list,\n",
    "    'Monthly Mean Precipitation': monthly_mean_precipitation_list\n",
    "})\n",
    "\n",
    "# Write to CSV\n",
    "result_df.to_csv('weather_data.csv', index=False)\n",
    "\n",
    "print(\"Data has been written to weather_data.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collection of Air Quality parameters like PM10, PM2.5, Carbon Monoxide, Nitrogen Dioxide, Sulphur Dioxide, Ozone, Dust, UV Index & Ammonia on monthly average basis through API.\n",
    "\n",
    "Every columns contains 12 space seperated values for each months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data extracted for The Hague\n",
      "Data extracted for Eindhoven\n",
      "Data extracted for Luxembourg\n",
      "Data extracted for Rotterdam\n",
      "Data extracted for Valencia\n",
      "Data extracted for Helsinki\n",
      "Data extracted for Amsterdam\n",
      "Data extracted for Vienna\n",
      "Data extracted for Munich\n",
      "Data extracted for Copenhagen\n",
      "Data extracted for Zurich\n",
      "Data extracted for Reykjavik\n",
      "Data extracted for Stuttgart\n",
      "Data extracted for Geneva\n",
      "Data extracted for Frankfurt\n",
      "Data extracted for Porto\n",
      "Data extracted for Madrid\n",
      "Data extracted for Gothenburg\n",
      "Data extracted for Tallinn\n",
      "Data extracted for Cologne\n",
      "Data extracted for Tokyo\n",
      "Data extracted for Edinburgh\n",
      "Data extracted for Muscat\n",
      "Data extracted for Oslo\n",
      "Data extracted for Glasgow\n",
      "Data extracted for Split\n",
      "Data extracted for Tampa\n",
      "Data extracted for Austin\n",
      "Data extracted for Cork\n",
      "Data extracted for Hamburg\n",
      "Data extracted for Charlotte\n",
      "Data extracted for Brisbane\n",
      "Data extracted for Orlando\n",
      "Data extracted for Kansas City\n",
      "Data extracted for Ljubljana\n",
      "Data extracted for Dallas\n",
      "Data extracted for Columbus\n",
      "Data extracted for Raleigh\n",
      "Data extracted for Seattle\n",
      "Data extracted for Minneapolis\n",
      "Data extracted for Vilnius\n",
      "Data extracted for Nashville\n",
      "Data extracted for Stockholm\n",
      "Data extracted for Indianapolis\n",
      "Data extracted for Tucson\n",
      "Data extracted for Brno\n",
      "Data extracted for Melbourne\n",
      "Data extracted for Portland\n",
      "Data extracted for Edmonton\n",
      "Data extracted for Perth\n",
      "Data extracted for Calgary\n",
      "Data extracted for San Diego\n",
      "Data extracted for Berlin\n",
      "Data extracted for Abu Dhabi\n",
      "Data extracted for San Antonio\n",
      "Data extracted for Sacramento\n",
      "Data extracted for Dubai\n",
      "Data extracted for Salt Lake City\n",
      "Data extracted for Atlanta\n",
      "Data extracted for Zagreb\n",
      "Data extracted for Pittsburgh\n",
      "Data extracted for Adelaide\n",
      "Data extracted for Boston\n",
      "Data extracted for Doha\n",
      "Data extracted for Prague\n",
      "Data extracted for San Jose\n",
      "Data extracted for Auckland\n",
      "Data extracted for Denver\n",
      "Data extracted for Sydney\n",
      "Data extracted for Houston\n",
      "Data extracted for Albuquerque\n",
      "Data extracted for Mississauga\n",
      "Data extracted for Wellington\n",
      "Data extracted for Singapore\n",
      "Data extracted for Canberra\n",
      "Data extracted for Vancouver\n",
      "Data extracted for Halifax\n",
      "Data extracted for Washington\n",
      "Data extracted for Honolulu\n",
      "Data extracted for Cape Town\n",
      "Data extracted for Birmingham\n",
      "Data extracted for Jeddah\n",
      "Data extracted for Phoenix\n",
      "Data extracted for Victoria\n",
      "Data extracted for Taipei\n",
      "Data extracted for Cluj-Napoca\n",
      "Data extracted for Miami\n",
      "Data extracted for Timisoara\n",
      "Data extracted for Montreal\n",
      "Data extracted for San Francisco\n",
      "Data extracted for Tel Aviv-Yafo\n",
      "Data extracted for Ottawa\n",
      "Data extracted for Lisbon\n",
      "Data extracted for Las Vegas\n",
      "Data extracted for Philadelphia\n",
      "Data extracted for Riyadh\n",
      "Data extracted for Toronto\n",
      "Data extracted for Quebec\n",
      "Data extracted for Riga\n",
      "Data extracted for Bratislava\n",
      "Data extracted for Nizhny Novgorod\n",
      "Data extracted for Chicago\n",
      "Data extracted for Bursa\n",
      "Data extracted for Manchester\n",
      "Data extracted for Brussels\n",
      "Data extracted for Pune\n",
      "Data extracted for Pretoria\n",
      "Data extracted for Los Angeles\n",
      "Data extracted for Barcelona\n",
      "Data extracted for Limassol\n",
      "Data extracted for Dublin\n",
      "Data extracted for Gdansk\n",
      "Data extracted for Seoul\n",
      "Data extracted for Varna\n",
      "Data extracted for Izmir\n",
      "Data extracted for Durban\n",
      "Data extracted for Montevideo\n",
      "Data extracted for Hyderabad\n",
      "Data extracted for Warsaw\n",
      "Data extracted for Winnipeg\n",
      "Data extracted for Wroclaw\n",
      "Data extracted for Islamabad\n",
      "Data extracted for Poznan\n",
      "Data extracted for Curitiba\n",
      "Data extracted for Johannesburg\n",
      "Data extracted for Brasilia\n",
      "Data extracted for New York\n",
      "Data extracted for Turin\n",
      "Data extracted for Bangalore\n",
      "Data extracted for Hong Kong\n",
      "Data extracted for Sofia\n",
      "Data extracted for Ankara\n",
      "Data extracted for Krakow\n",
      "Data extracted for Chennai\n",
      "Data extracted for Novi Sad\n",
      "Data extracted for Rome\n",
      "Data extracted for Budapest\n",
      "Data extracted for London\n",
      "Data extracted for Sarajevo\n",
      "Data extracted for Kuwait City\n",
      "Data extracted for Lviv\n",
      "Data extracted for Minsk\n",
      "Data extracted for Paris\n",
      "Data extracted for Kuala Lumpur\n",
      "Data extracted for Bucharest\n",
      "Data extracted for Moscow\n",
      "Data extracted for Gurgaon\n",
      "Data extracted for Monterrey\n",
      "Data extracted for Guadalajara\n",
      "Data extracted for Ahmedabad\n",
      "Data extracted for Amman\n",
      "Data extracted for Athens\n",
      "Data extracted for Yerevan\n",
      "Data extracted for Porto Alegre\n",
      "Data extracted for Panama City\n",
      "Data extracted for Medellin\n",
      "Data extracted for Tbilisi\n",
      "Data extracted for Thessaloniki\n",
      "Data extracted for Milan\n",
      "Data extracted for Baku\n",
      "Data extracted for Istanbul\n",
      "Data extracted for Quito\n",
      "Data extracted for Belo Horizonte\n",
      "Data extracted for Belgrade\n",
      "Data extracted for Saint Petersburg\n",
      "Data extracted for Odessa\n",
      "Data extracted for Beijing\n",
      "Data extracted for Kharkiv\n",
      "Data extracted for Lahore\n",
      "Data extracted for Delhi\n",
      "Data extracted for Skopje\n",
      "Data extracted for Kiev\n",
      "Data extracted for Kolkata\n",
      "Data extracted for Santiago\n",
      "Data extracted for Buenos Aires\n",
      "Data extracted for Hanoi\n",
      "Data extracted for Nairobi\n",
      "Data extracted for Dnipro\n",
      "Data extracted for Shanghai\n",
      "Data extracted for Tirana\n",
      "Data extracted for Yekaterinburg\n",
      "Data extracted for Almaty\n",
      "Data extracted for Bogota\n",
      "Data extracted for Novosibirsk\n",
      "Data extracted for Mexico City\n",
      "Data extracted for Bangkok\n",
      "Data extracted for Karachi\n",
      "Data extracted for Jakarta\n",
      "Data extracted for Lima\n",
      "Data extracted for Mumbai\n",
      "Data extracted for Sao Paulo\n",
      "Data extracted for Colombo\n",
      "Data extracted for Cairo\n",
      "Data extracted for Ho Chi Minh City\n",
      "Data extracted for Rio de Janeiro\n",
      "Data extracted for Caracas\n",
      "Data extracted for Beirut\n",
      "Data extracted for Dhaka\n",
      "Data extracted for Tehran\n",
      "Data extracted for Manila\n",
      "Data extracted for Lagos\n"
     ]
    }
   ],
   "source": [
    "import openmeteo_requests\n",
    "import requests_cache\n",
    "from retry_requests import retry\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Define a function to calculate monthly averages\n",
    "def calculate_monthly_averages(latitude, longitude):\n",
    "    # Setup the Open-Meteo API client with cache and retry on error\n",
    "    cache_session = requests_cache.CachedSession('.cache', expire_after=3600)\n",
    "    retry_session = retry(cache_session, retries=5, backoff_factor=0.2)\n",
    "    openmeteo = openmeteo_requests.Client(session=retry_session)\n",
    "\n",
    "    url = \"https://air-quality-api.open-meteo.com/v1/air-quality\"\n",
    "    params = {\n",
    "        \"latitude\": latitude,\n",
    "        \"longitude\": longitude,\n",
    "        \"hourly\": [\"pm10\", \"pm2_5\", \"carbon_monoxide\", \"nitrogen_dioxide\", \"sulphur_dioxide\", \"ozone\", \"dust\", \"uv_index\", \"ammonia\"],\n",
    "        \"start_date\": \"2023-01-01\",\n",
    "        \"end_date\": \"2023-12-31\"\n",
    "    }\n",
    "\n",
    "    responses = openmeteo.weather_api(url, params=params)\n",
    "\n",
    "    # Process hourly data for the first location\n",
    "    response = responses[0]\n",
    "    hourly = response.Hourly()\n",
    "    hourly_data = {\n",
    "        \"pm10\": hourly.Variables(0).ValuesAsNumpy(),\n",
    "        \"pm2_5\": hourly.Variables(1).ValuesAsNumpy(),\n",
    "        \"carbon_monoxide\": hourly.Variables(2).ValuesAsNumpy(),\n",
    "        \"nitrogen_dioxide\": hourly.Variables(3).ValuesAsNumpy(),\n",
    "        \"sulphur_dioxide\": hourly.Variables(4).ValuesAsNumpy(),\n",
    "        \"ozone\": hourly.Variables(5).ValuesAsNumpy(),\n",
    "        \"dust\": hourly.Variables(6).ValuesAsNumpy(),\n",
    "        \"uv_index\": hourly.Variables(7).ValuesAsNumpy(),\n",
    "        \"ammonia\": hourly.Variables(8).ValuesAsNumpy(),\n",
    "        \"date\": pd.date_range(start=pd.to_datetime(hourly.Time(), unit=\"s\", utc=True),\n",
    "                              end=pd.to_datetime(hourly.TimeEnd(), unit=\"s\", utc=True),\n",
    "                              freq=pd.Timedelta(seconds=hourly.Interval()),\n",
    "                              inclusive=\"left\")\n",
    "    }\n",
    "\n",
    "    hourly_dataframe = pd.DataFrame(data=hourly_data)\n",
    "    hourly_dataframe['date'] = pd.to_datetime(hourly_dataframe['date'])\n",
    "    hourly_dataframe['month'] = hourly_dataframe['date'].dt.month\n",
    "    monthly_means = hourly_dataframe.groupby('month').mean(numeric_only=False)  # Specify numeric_only=False\n",
    "\n",
    "    return monthly_means\n",
    "\n",
    "# Read city information from CSV\n",
    "city_info = pd.read_csv('city_information.csv', usecols=['City', 'Latitude', 'Longitude'])\n",
    "\n",
    "# Create an empty DataFrame for output\n",
    "output_df = pd.DataFrame(columns=['City', 'Monthly Average pm10', 'Monthly Average pm2.5',\n",
    "                                  'Monthly Average carbon_monoxide', 'Monthly Average nitrogen_dioxide',\n",
    "                                  'Monthly Average sulphur_dioxide', 'Monthly Average ozone',\n",
    "                                  'Monthly Average dust', 'Monthly Average uv_index', 'Monthly Average ammonia'])\n",
    "\n",
    "# Loop through each row of city information\n",
    "for index, row in city_info.iterrows():\n",
    "    city = row['City']\n",
    "    latitude = row['Latitude']\n",
    "    longitude = row['Longitude']\n",
    "    \n",
    "    # Calculate monthly averages for the current city\n",
    "    monthly_averages = calculate_monthly_averages(latitude, longitude)\n",
    "    \n",
    "    # Append monthly average values to the output DataFrame\n",
    "    new_row = {'City': city,\n",
    "           'Monthly Average pm10': ' '.join(map(str, monthly_averages['pm10'].tolist())),\n",
    "           'Monthly Average pm2.5': ' '.join(map(str, monthly_averages['pm2_5'].tolist())),\n",
    "           'Monthly Average carbon_monoxide': ' '.join(map(str, monthly_averages['carbon_monoxide'].tolist())),\n",
    "           'Monthly Average nitrogen_dioxide': ' '.join(map(str, monthly_averages['nitrogen_dioxide'].tolist())),\n",
    "           'Monthly Average sulphur_dioxide': ' '.join(map(str, monthly_averages['sulphur_dioxide'].tolist())),\n",
    "           'Monthly Average ozone': ' '.join(map(str, monthly_averages['ozone'].tolist())),\n",
    "           'Monthly Average dust': ' '.join(map(str, monthly_averages['dust'].tolist())),\n",
    "           'Monthly Average uv_index': ' '.join(map(str, monthly_averages['uv_index'].tolist())),\n",
    "           'Monthly Average ammonia': ' '.join(map(str, monthly_averages['ammonia'].tolist()))}\n",
    "    time.sleep(1)\n",
    "    print(f'Data extracted for {city}')\n",
    "\n",
    "    # Concatenate the new row to the output DataFrame\n",
    "    output_df = pd.concat([output_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "# Write output to CSV\n",
    "output_df.to_csv('air_quality_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final task to merge 4 csv files to complete the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged CSV file 'city_profile.csv' has been created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Read each CSV file\n",
    "quality_of_life_df = pd.read_csv(\"modified_quality_of_life_indices.csv\")\n",
    "city_info_df = pd.read_csv(\"city_information.csv\")\n",
    "air_quality_df = pd.read_csv(\"air_quality_data.csv\")\n",
    "weather_df = pd.read_csv(\"weather_data.csv\")\n",
    "\n",
    "# Merge dataframes based on the 'City' column\n",
    "merged_df = quality_of_life_df.merge(city_info_df, on='City') \\\n",
    "    .merge(air_quality_df, on='City') \\\n",
    "    .merge(weather_df, on='City')\n",
    "\n",
    "# Write merged dataframe to a new CSV file\n",
    "merged_df.to_csv(\"city_profile.csv\", index=False)\n",
    "\n",
    "print(\"Merged CSV file 'city_profile.csv' has been created successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
